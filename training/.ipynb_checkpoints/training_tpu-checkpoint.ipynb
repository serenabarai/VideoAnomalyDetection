{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 983,
     "status": "ok",
     "timestamp": 1584779353565,
     "user": {
      "displayName": "VAIBHAV GOYAL",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GinDWpqhe1cS7dAHJ185tE50Yn3I6rIULUmtpCN=s64",
      "userId": "09106313311714801771"
     },
     "user_tz": -330
    },
    "id": "WA5QuqTVTsRs",
    "outputId": "4eb4423f-0261-4ef2-da77-68eb1449afba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "import os\n",
    "os.chdir(\"/content/gdrive/My Drive/BTP-SEM-8/VideoAnomalyDetection/training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3491,
     "status": "ok",
     "timestamp": 1584779357596,
     "user": {
      "displayName": "VAIBHAV GOYAL",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GinDWpqhe1cS7dAHJ185tE50Yn3I6rIULUmtpCN=s64",
      "userId": "09106313311714801771"
     },
     "user_tz": -330
    },
    "id": "ThgmofE9rjbM",
    "outputId": "3fbbffe3-8594-4e52-8478-449e4fc1b845"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'theano'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9e5278b1fa76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'theano'"
     ]
    }
   ],
   "source": [
    "# %tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, Dropout, Activation,LSTM,Reshape\n",
    "from tensorflow.python.keras.regularizers import l2\n",
    "# from tensorflow.train import AdamOptimizer, AdagradOptimizer\n",
    "# from tensorflow.optimizers import Adagrad\n",
    "from scipy.io import loadmat, savemat\n",
    "from tensorflow.python.keras.models import model_from_json\n",
    "from tensorflow.keras.models import load_model\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import csv\n",
    "import collections\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "from os import listdir\n",
    "import skimage.transform\n",
    "from skimage import color\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import numpy\n",
    "from datetime import datetime\n",
    "from os.path import basename\n",
    "import glob\n",
    "import theano.sandbox\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3079,
     "status": "ok",
     "timestamp": 1584779357598,
     "user": {
      "displayName": "VAIBHAV GOYAL",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GinDWpqhe1cS7dAHJ185tE50Yn3I6rIULUmtpCN=s64",
      "userId": "09106313311714801771"
     },
     "user_tz": -330
    },
    "id": "JamxqfZcDWOd",
    "outputId": "51e564f7-c78f-4cb6-aca8-0ced288f375a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2714,
     "status": "ok",
     "timestamp": 1584779357599,
     "user": {
      "displayName": "VAIBHAV GOYAL",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GinDWpqhe1cS7dAHJ185tE50Yn3I6rIULUmtpCN=s64",
      "userId": "09106313311714801771"
     },
     "user_tz": -330
    },
    "id": "Zwoss6gGTsR4",
    "outputId": "74756d63-f46f-4449-98a1-5296906dc937"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Model\n"
     ]
    }
   ],
   "source": [
    "print(\"Create Model\")\n",
    "\n",
    "def create_model(batch_size = 64):\n",
    "    inputs = Input(shape=(4096), batch_size=batch_size)\n",
    "    layer1 = Dense(512, kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001),activation='relu')(inputs)\n",
    "    layer1 = Dropout(0.6)(layer1)\n",
    "    layer2 = Dense(32, kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001))(layer1)\n",
    "    layer2 = Dropout(0.6)(layer2)\n",
    "    outputs = Dense(1, kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001),activation='sigmoid')(layer2)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2978,
     "status": "ok",
     "timestamp": 1584779358239,
     "user": {
      "displayName": "VAIBHAV GOYAL",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GinDWpqhe1cS7dAHJ185tE50Yn3I6rIULUmtpCN=s64",
      "userId": "09106313311714801771"
     },
     "user_tz": -330
    },
    "id": "AndDzZlSLiWS",
    "outputId": "3616150a-62f2-4a99-d893-11f0b3a75177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Model\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "print(\"Create Model\")\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=4096,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001),activation='relu'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(32,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(1,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001),activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fd9sDmEWTsR_"
   },
   "outputs": [],
   "source": [
    "def load_model(json_path): # Function to load the model\n",
    "    model = model_from_json(open(json_path).read())\n",
    "    return model\n",
    "\n",
    "def load_weights(model, weight_path): # Function to load the model weights\n",
    "    dict2 = loadmat(weight_path)\n",
    "    dict = conv_dict(dict2)\n",
    "    i = 0\n",
    "    for layer in model.layers:\n",
    "        weights = dict[str(i)]\n",
    "        layer.set_weights(weights)\n",
    "        i += 1\n",
    "    return model\n",
    "\n",
    "def conv_dict(dict2):\n",
    "    i = 0\n",
    "    dict = {}\n",
    "    for i in range(len(dict2)):\n",
    "        if str(i) in dict2:\n",
    "            if dict2[str(i)].shape == (0, 0):\n",
    "                dict[str(i)] = dict2[str(i)]\n",
    "            else:\n",
    "                weights = dict2[str(i)][0]\n",
    "                weights2 = []\n",
    "                for weight in weights:\n",
    "                    if weight.shape in [(1, x) for x in range(0, 5000)]:\n",
    "                        weights2.append(weight[0])\n",
    "                    else:\n",
    "                        weights2.append(weight)\n",
    "                dict[str(i)] = weights2\n",
    "    return dict\n",
    "\n",
    "def save_model(savemodel, json_path, weight_path): # Function to save the model\n",
    "    json_string = model.to_json()\n",
    "    open(json_path, 'w').write(json_string)\n",
    "    dict = {}\n",
    "    i = 0\n",
    "    for layer in model.layers:\n",
    "        weights = layer.get_weights()\n",
    "        my_list = np.zeros(len(weights), dtype=np.object)\n",
    "        my_list[:] = weights\n",
    "        dict[str(i)] = my_list\n",
    "        i += 1\n",
    "    savemat(weight_path, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HTYFfL6PTsSD"
   },
   "outputs": [],
   "source": [
    "# Load Training Dataset\n",
    "\n",
    "def load_dataset_Train_batch(abnormal_videos_matrix, normal_videos_matrix, ab_idx, n_idx, batch_size):\n",
    "#    print(\"Loading training batch\")\n",
    "\n",
    "    AllFeatures = []  # To store Cad3D features of a batch\n",
    "\n",
    "    Video_count=-1\n",
    "    for iv in range(ab_idx*batch_size, ab_idx*batch_size + batch_size):\n",
    "        Video_count=Video_count+1\n",
    "        words = abnormal_videos_matrix[iv]\n",
    "        num_feat = int(len(words) / 4096)\n",
    "        # Number of features per video to be loaded. In our case num_feat=32, as we divide the video into 32 segments. Note that\n",
    "        # we have already computed C3D features for the whole video and divide the video features into 32 segments. Please see Save_C3DFeatures_32Segments.m as well\n",
    "\n",
    "        count = -1\n",
    "        VideoFeatues = []\n",
    "        for feat in range(0, num_feat):\n",
    "            feat_row1 = np.float32(words[feat * 4096:feat * 4096 + 4096])\n",
    "            count = count + 1\n",
    "            if count == 0:\n",
    "                VideoFeatues = feat_row1\n",
    "            if count > 0:\n",
    "                VideoFeatues = np.vstack((VideoFeatues, feat_row1))\n",
    "\n",
    "        if Video_count == 0:\n",
    "            AllFeatures = VideoFeatues\n",
    "        if Video_count > 0:\n",
    "            AllFeatures = np.vstack((AllFeatures, VideoFeatues))\n",
    "        # print(\" Abnormal Features  loaded\")\n",
    "\n",
    "        \n",
    "        \n",
    "    #print(\"Loading Normal videos...\")\n",
    "\n",
    "    for iv in range(n_idx*batch_size, n_idx*batch_size + batch_size):\n",
    "        words = normal_videos_matrix[iv]\n",
    "        feat_row1 = np.array([])\n",
    "        num_feat = int(len(words) /4096)   # Number of features to be loaded. In our case num_feat=32, as we divide the video into 32 segments.\n",
    "\n",
    "        count = -1\n",
    "        VideoFeatues = []\n",
    "        for feat in range(0, num_feat):\n",
    "\n",
    "\n",
    "            feat_row1 = np.float32(words[feat * 4096:feat * 4096 + 4096])\n",
    "            count = count + 1\n",
    "            if count == 0:\n",
    "                VideoFeatues = feat_row1\n",
    "            if count > 0:\n",
    "                VideoFeatues = np.vstack((VideoFeatues, feat_row1))\n",
    "            feat_row1 = []\n",
    "        AllFeatures = np.vstack((AllFeatures, VideoFeatues))\n",
    "\n",
    "    #print(\"Features  loaded\")\n",
    "\n",
    "    AllLabels = np.ones((32*32)) #32 videos and 32 segments\n",
    "    AllLabels = np.concatenate((AllLabels, np.zeros((32*32))))\n",
    "\n",
    "\n",
    "    return  AllFeatures,AllLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_gMCPgVOaKpZ"
   },
   "outputs": [],
   "source": [
    "# Load Testing Dataset\n",
    "\n",
    "def load_dataset_Test_batch(abnormal_videos_matrix, normal_videos_matrix):\n",
    "#    print(\"Loading training batch\")\n",
    "\n",
    "    #batchsize=60       # Each batch contain 60 videos.\n",
    "    #n_exp=int(batchsize/2)  # Number of abnormal and normal videos\n",
    "\n",
    "    #Num_abnormal = 810  # Total number of abnormal videos in Training Dataset.\n",
    "    #Num_Normal = 800    # Total number of Normal videos in Training Dataset.\n",
    "\n",
    "\n",
    "    # We assume the features of abnormal videos and normal videos are located in two different folders.\n",
    "    #Abnor_list_iter = np.random.permutation(Num_abnormal)\n",
    "    #Abnor_list_iter = Abnor_list_iter[Num_abnormal-n_exp:] # Indexes for randomly selected Abnormal Videos\n",
    "    #Norm_list_iter = np.random.permutation(Num_Normal)\n",
    "    #Norm_list_iter = Norm_list_iter[Num_Normal-n_exp:]     # Indexes for randomly selected Normal Videos\n",
    "\n",
    "\n",
    "    AllFeatures = []  # To store Cad3D features of all testing videos\n",
    "    #print(\"Loading Abnormal videos Features...\")\n",
    "\n",
    "    Video_count=-1\n",
    "    for iv in range(len(abnormal_videos_matrix)):\n",
    "        Video_count=Video_count+1\n",
    "        words = abnormal_videos_matrix[iv]\n",
    "        num_feat = int(len(words) / 4096)\n",
    "        # Number of features per video to be loaded. In our case num_feat=32, as we divide the video into 32 segments. Note that\n",
    "        # we have already computed C3D features for the whole video and divide the video features into 32 segments. Please see Save_C3DFeatures_32Segments.m as well\n",
    "\n",
    "        count = -1\n",
    "        VideoFeatues = []\n",
    "        for feat in range(0, num_feat):\n",
    "            feat_row1 = np.float32(words[feat * 4096:feat * 4096 + 4096])\n",
    "            count = count + 1\n",
    "            if count == 0:\n",
    "                VideoFeatues = feat_row1\n",
    "            if count > 0:\n",
    "                VideoFeatues = np.vstack((VideoFeatues, feat_row1))\n",
    "\n",
    "        if Video_count == 0:\n",
    "            AllFeatures = VideoFeatues\n",
    "        if Video_count > 0:\n",
    "            AllFeatures = np.vstack((AllFeatures, VideoFeatues))\n",
    "        # print(\" Abnormal Features  loaded\")\n",
    "\n",
    "        \n",
    "        \n",
    "    #print(\"Loading Normal videos...\")\n",
    "\n",
    "    for iv in range(len(normal_videos_matrix)):\n",
    "        words = normal_videos_matrix[iv]\n",
    "        feat_row1 = np.array([])\n",
    "        num_feat = int(len(words) /4096)   # Number of features to be loaded. In our case num_feat=32, as we divide the video into 32 segments.\n",
    "\n",
    "        count = -1\n",
    "        VideoFeatues = []\n",
    "        for feat in range(0, num_feat):\n",
    "\n",
    "\n",
    "            feat_row1 = np.float32(words[feat * 4096:feat * 4096 + 4096])\n",
    "            count = count + 1\n",
    "            if count == 0:\n",
    "                VideoFeatues = feat_row1\n",
    "            if count > 0:\n",
    "                VideoFeatues = np.vstack((VideoFeatues, feat_row1))\n",
    "            feat_row1 = []\n",
    "        AllFeatures = np.vstack((AllFeatures, VideoFeatues))\n",
    "\n",
    "    #print(\"Features  loaded\")\n",
    "\n",
    "\n",
    "    AllLabels = np.zeros(len(abnormal_videos_matrix) + len(normal_videos_matrix), dtype='uint8')\n",
    "    th_loop1=len(abnormal_videos_matrix)\n",
    "    th_loop2=len(abnormal_videos_matrix)-1\n",
    "\n",
    "\n",
    "\n",
    "    for iv in range(0, len(abnormal_videos_matrix) + len(normal_videos_matrix)):\n",
    "            if iv< th_loop1:\n",
    "                AllLabels[iv] = int(0)  # All instances of abnormal videos are labeled 0.  This will be used in custom_objective to keep track of normal and abnormal videos indexes.\n",
    "            if iv > th_loop2:\n",
    "                AllLabels[iv] = int(1)   # All instances of Normal videos are labeled 1. This will be used in custom_objective to keep track of normal and abnormal videos indexes.\n",
    "           # print(\"ALLabels  loaded\")\n",
    "\n",
    "    return  AllFeatures,AllLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j18O_BfNCvs5"
   },
   "outputs": [],
   "source": [
    "#Tensorflow\n",
    "def custom_objective(y_true, y_pred):\n",
    "    'Custom Objective function'\n",
    "\n",
    "    #print(type(y_true))\n",
    "    y_true = tf.keras.backend.flatten(y_true)\n",
    "    y_pred = tf.keras.backend.flatten(y_pred)\n",
    "\n",
    "    n_seg = 32  # Because we have 32 segments per video.\n",
    "    nvid = 64\n",
    "    n_exp = int(nvid/2)\n",
    "    Num_d=32*nvid\n",
    "\n",
    "\n",
    "    sub_max = tf.ones_like(y_pred) # sub_max represents the highest scoring instants in bags (videos).\n",
    "    sub_sum_labels = tf.ones_like(y_true) # It is used to sum the labels in order to distinguish between normal and abnormal videos.\n",
    "    sub_sum_l1=tf.ones_like(y_true, dtype='float32')  # For holding the concatenation of summation of scores in the bag.\n",
    "    sub_l2 = tf.ones_like(y_true, dtype='float32') # For holding the concatenation of L2 of score in the bag.\n",
    "\n",
    "    for ii in range(0, nvid, 1):\n",
    "        # For Labels\n",
    "        mm = y_true[ii * n_seg:ii * n_seg + n_seg]\n",
    "        sub_sum_labels = tf.concat([sub_sum_labels, tf.stack([tf.math.reduce_sum(mm)])],0) # Just to keep track of abnormal and normal vidoes\n",
    "        # For Features scores\n",
    "        Feat_Score = y_pred[ii * n_seg:ii * n_seg + n_seg]\n",
    "        sub_max = tf.concat([sub_max, tf.stack([tf.math.reduce_max(Feat_Score)])],0)         # Keep the maximum score of scores of all instances in a Bag (video)\n",
    "        sub_sum_l1 = tf.concat([sub_sum_l1, tf.stack([tf.math.reduce_sum(Feat_Score)])],0)   # Keep the sum of scores of all instances in a Bag (video)\n",
    "\n",
    "        z1 = tf.ones_like(Feat_Score)\n",
    "        z2 = tf.concat([z1, Feat_Score],0)\n",
    "        z3 = tf.concat([Feat_Score, z1],0)\n",
    "        z_22 = z2[31:]\n",
    "        z_44 = z3[:33]\n",
    "        z = tf.subtract(z_22,z_44)\n",
    "        # print(\"shape ypred:\", y_pred.shape)\n",
    "        # print(\"shape feat_score:\", Feat_Score.shape)\n",
    "        # print(\"shape z1:\", z1.shape)\n",
    "        # print(\"shape z2:\", z2.shape)\n",
    "        # print(\"shape z3:\", z3.shape)\n",
    "        # print(\"shape z_22:\", z_22.shape)\n",
    "        # print(\"shape z_44:\", z_44.shape)\n",
    "        z = tf.math.reduce_sum(tf.math.square(z))\n",
    "        sub_l2 = tf.concat([sub_l2, tf.stack([z])],0)\n",
    "\n",
    "\n",
    "    # sub_max[Num_d:] means include all elements after Num_d.\n",
    "    # AllLabels =[2 , 4, 3 ,9 ,6 ,12,7 ,18 ,9 ,14]\n",
    "    # z=x[4:]\n",
    "    #[  6.  12.   7.  18.   9.  14.]\n",
    "\n",
    "    sub_score = sub_max[Num_d:]  # We need this step since we have used tf.ones_like\n",
    "    F_labels = sub_sum_labels[Num_d:] # We need this step since we have used tf.ones_like\n",
    "    #  F_labels contains integer 32 for normal video and 0 for abnormal videos. This because of labeling done at the end of \"load_dataset_Train_batch\"\n",
    "\n",
    "\n",
    "\n",
    "    # AllLabels =[2 , 4, 3 ,9 ,6 ,12,7 ,18 ,9 ,14]\n",
    "    # z=x[:4]\n",
    "    # [ 2 4 3 9]... This shows 0 to 3 elements\n",
    "\n",
    "    sub_sum_l1 = sub_sum_l1[Num_d:] # We need this step since we have used tf.ones_like\n",
    "    sub_sum_l1 = sub_sum_l1[:n_exp]\n",
    "    sub_l2 = sub_l2[Num_d:]         # We need this step since we have used tf.ones_like\n",
    "    sub_l2 = sub_l2[:n_exp]\n",
    "\n",
    "\n",
    "    #indx_nor = theano.tensor.eq(F_labels, 32).nonzero()[0]  # Index of normal videos: Since we labeled 1 for each of 32 segments of normal videos F_labels=32 for normal video\n",
    "    #indx_abn = theano.tensor.eq(F_labels, 0).nonzero()[0]\n",
    "\n",
    "    indx_nor = tf.squeeze(tf.where(tf.equal(F_labels,0)))\n",
    "    indx_abn = tf.squeeze(tf.where(tf.equal(F_labels,32)))\n",
    "\n",
    "    #print(\"Index_nor:\", indx_nor.shape)\n",
    "    #print(\"sub_score:\", sub_score.shape)\n",
    "\n",
    "    n_Nor=n_exp\n",
    "\n",
    "    #Sub_Nor = sub_score[indx_nor] # Maximum Score for each of abnormal video\n",
    "    #Sub_Abn = sub_score[indx_abn] # Maximum Score for each of normal video\n",
    "    Sub_Nor = tf.gather(sub_score, indx_nor) \n",
    "    #print(\"Sub_Nor:\", Sub_Nor.shape)\n",
    "    Sub_Abn = tf.gather(sub_score, indx_abn) \n",
    "\n",
    "    z = tf.ones_like(y_true, dtype='float32')\n",
    "    for ii in range(0, n_Nor, 1):\n",
    "        sub_z = tf.math.maximum(1 - Sub_Abn + Sub_Nor[ii], 0)\n",
    "        z = tf.concat([z, tf.stack([tf.math.reduce_sum(sub_z)])],0)\n",
    "\n",
    "    z = z[Num_d:]  # We need this step since we have used tf.ones_like\n",
    "    z = tf.math.reduce_mean(z, axis=-1) +  0.00008*tf.math.reduce_sum(sub_sum_l1) + 0.00008*tf.math.reduce_sum(sub_l2)  # Final Loss f\n",
    "    #print(\"Loss Z:\", z)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzOPq-jcZ5xR"
   },
   "outputs": [],
   "source": [
    "def generate_permutation(num, length):\n",
    "    if num > length:\n",
    "        return np.random.permutation(num)[:length]\n",
    "\n",
    "    perm = None\n",
    "    for i in range(0,int(length/num)):\n",
    "        if i == 0:\n",
    "            perm = np.random.permutation(num)\n",
    "        else:\n",
    "            perm = np.concatenate((perm, np.random.permutation(num)))\n",
    "    \n",
    "    if len(perm) < length:\n",
    "        req = length - len(perm)\n",
    "        perm = np.concatenate((perm, np.random.permutation(num)[:req]))\n",
    "    \n",
    "    return perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mS_LGriOV10i",
    "outputId": "59dd5e7a-dad8-499a-a8f7-a76bbe4270fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "abnormal batches: 26\n",
      "normal batches 25\n",
      "These epochs=981) took: 0:00:27.800109, with loss of 2.3468685\n",
      "These epochs=982) took: 0:00:39.867540, with loss of 2.2296379\n",
      "These epochs=983) took: 0:00:51.839008, with loss of 3.1824527\n",
      "These epochs=984) took: 0:01:03.807390, with loss of 3.1693535\n",
      "These epochs=985) took: 0:01:15.673199, with loss of 2.2312164\n",
      "These epochs=986) took: 0:01:27.443335, with loss of 3.1675725\n",
      "These epochs=987) took: 0:01:39.471697, with loss of 3.258409\n",
      "These epochs=988) took: 0:01:51.455768, with loss of 4.2093167\n",
      "These epochs=989) took: 0:02:03.487878, with loss of 2.1171224\n",
      "These epochs=990) took: 0:02:15.537590, with loss of 3.1309404\n",
      "These epochs=991) took: 0:02:27.492566, with loss of 2.1601853\n",
      "These epochs=992) took: 0:02:39.667038, with loss of 2.2877438\n",
      "These epochs=993) took: 0:02:51.694354, with loss of 1.2367845\n",
      "These epochs=994) took: 0:03:03.690883, with loss of 5.155427\n",
      "These epochs=995) took: 0:03:15.822682, with loss of 4.1345444\n",
      "These epochs=996) took: 0:03:27.672160, with loss of 2.2555778\n",
      "These epochs=997) took: 0:03:39.686742, with loss of 3.1381345\n",
      "These epochs=998) took: 0:03:51.954170, with loss of 2.175705\n",
      "These epochs=999) took: 0:04:03.930670, with loss of 3.1303353\n",
      "These epochs=1000) took: 0:04:15.922826, with loss of 4.1196\n",
      "These epochs=1001) took: 0:04:28.179823, with loss of 2.1791852\n",
      "These epochs=1002) took: 0:04:40.350985, with loss of 2.11236\n",
      "These epochs=1003) took: 0:04:52.477943, with loss of 4.175124\n",
      "These epochs=1004) took: 0:05:04.459905, with loss of 4.075401\n",
      "These epochs=1005) took: 0:05:16.630162, with loss of 3.1744652\n",
      "These epochs=1006) took: 0:05:28.723969, with loss of 2.3241413\n",
      "These epochs=1007) took: 0:05:40.714863, with loss of 2.1991735\n",
      "These epochs=1008) took: 0:05:52.724809, with loss of 2.2011847\n",
      "These epochs=1009) took: 0:06:04.787418, with loss of 2.2335804\n",
      "These epochs=1010) took: 0:06:16.926867, with loss of 4.1814833\n",
      "These epochs=1011) took: 0:06:28.877369, with loss of 2.1413808\n",
      "These epochs=1012) took: 0:06:40.875881, with loss of 1.2145926\n",
      "These epochs=1013) took: 0:06:52.903560, with loss of 2.3326802\n",
      "These epochs=1014) took: 0:07:04.910203, with loss of 1.1076984\n",
      "These epochs=1015) took: 0:07:17.053171, with loss of 3.5274305\n",
      "These epochs=1016) took: 0:07:29.137230, with loss of 3.1843922\n",
      "These epochs=1017) took: 0:07:41.332742, with loss of 5.2360764\n",
      "These epochs=1018) took: 0:07:53.562740, with loss of 3.1103816\n",
      "These epochs=1019) took: 0:08:05.684852, with loss of 2.118575\n",
      "These epochs=1020) took: 0:08:17.842443, with loss of 3.1154883\n",
      "These epochs=1021) took: 0:08:29.876001, with loss of 3.0826778\n",
      "These epochs=1022) took: 0:08:41.991919, with loss of 4.0853577\n",
      "These epochs=1023) took: 0:08:53.968052, with loss of 1.2382923\n",
      "These epochs=1024) took: 0:09:05.982258, with loss of 3.1918428\n",
      "These epochs=1025) took: 0:09:17.975990, with loss of 1.0858777\n",
      "These epochs=1026) took: 0:09:29.823880, with loss of 5.080663\n",
      "These epochs=1027) took: 0:09:41.787315, with loss of 5.161703\n",
      "These epochs=1028) took: 0:09:53.762784, with loss of 2.1062508\n",
      "These epochs=1029) took: 0:10:05.847398, with loss of 1.0795108\n",
      "These epochs=1030) took: 0:10:18.039079, with loss of 3.1134822\n",
      "These epochs=1031) took: 0:10:30.276151, with loss of 4.137784\n",
      "These epochs=1032) took: 0:10:42.235254, with loss of 4.1698155\n",
      "These epochs=1033) took: 0:10:54.271076, with loss of 1.2546318\n",
      "These epochs=1034) took: 0:11:06.328751, with loss of 4.151488\n",
      "These epochs=1035) took: 0:11:18.344064, with loss of 3.2809825\n",
      "These epochs=1036) took: 0:11:30.373035, with loss of 2.1410384\n",
      "These epochs=1037) took: 0:11:42.417518, with loss of 1.118809\n",
      "These epochs=1038) took: 0:11:54.524846, with loss of 1.0896559\n",
      "These epochs=1039) took: 0:12:06.757479, with loss of 4.0980616\n",
      "These epochs=1040) took: 0:12:18.823100, with loss of 4.1955376\n",
      "These epochs=1041) took: 0:12:30.992469, with loss of 4.0909147\n",
      "These epochs=1042) took: 0:12:43.254039, with loss of 2.3118334\n",
      "These epochs=1043) took: 0:12:55.404641, with loss of 1.1498849\n",
      "These epochs=1044) took: 0:13:07.731561, with loss of 2.1262527\n",
      "These epochs=1045) took: 0:13:19.747085, with loss of 3.1830223\n",
      "These epochs=1046) took: 0:13:31.857826, with loss of 3.2060328\n",
      "These epochs=1047) took: 0:13:44.092408, with loss of 3.124842\n",
      "These epochs=1048) took: 0:13:56.165885, with loss of 5.180382\n",
      "These epochs=1049) took: 0:14:08.340220, with loss of 4.2042866\n",
      "These epochs=1050) took: 0:14:20.351576, with loss of 3.0931017\n",
      "These epochs=1051) took: 0:14:32.466530, with loss of 4.1336403\n",
      "These epochs=1052) took: 0:14:44.489465, with loss of 6.2160187\n",
      "These epochs=1053) took: 0:14:56.450152, with loss of 2.2321815\n",
      "These epochs=1054) took: 0:15:08.321398, with loss of 4.0740266\n",
      "These epochs=1055) took: 0:15:20.256846, with loss of 4.2241254\n",
      "These epochs=1056) took: 0:15:32.100377, with loss of 5.118316\n",
      "These epochs=1057) took: 0:15:44.013312, with loss of 2.1034515\n",
      "These epochs=1058) took: 0:15:56.014207, with loss of 4.2148323\n",
      "These epochs=1059) took: 0:16:07.942165, with loss of 2.1700726\n",
      "These epochs=1060) took: 0:16:19.822409, with loss of 2.1969814\n",
      "These epochs=1061) took: 0:16:31.838873, with loss of 4.1375628\n",
      "These epochs=1062) took: 0:16:43.704572, with loss of 3.1277304\n",
      "These epochs=1063) took: 0:16:55.579080, with loss of 2.140094\n",
      "These epochs=1064) took: 0:17:07.475836, with loss of 4.161658\n",
      "These epochs=1065) took: 0:17:19.476460, with loss of 3.1246672\n",
      "These epochs=1066) took: 0:17:31.439989, with loss of 4.2237716\n",
      "These epochs=1067) took: 0:17:43.435302, with loss of 1.1269505\n",
      "These epochs=1068) took: 0:17:55.476007, with loss of 4.066839\n",
      "These epochs=1069) took: 0:18:07.513244, with loss of 6.0780106\n",
      "These epochs=1070) took: 0:18:19.611450, with loss of 6.1690264\n",
      "These epochs=1071) took: 0:18:31.588483, with loss of 3.1247122\n",
      "These epochs=1072) took: 0:18:43.717322, with loss of 2.2831337\n",
      "These epochs=1073) took: 0:18:55.772027, with loss of 2.3725688\n",
      "These epochs=1074) took: 0:19:07.682177, with loss of 3.233162\n",
      "These epochs=1075) took: 0:19:19.610626, with loss of 2.5683982\n",
      "These epochs=1076) took: 0:19:31.587361, with loss of 2.2238445\n",
      "These epochs=1077) took: 0:19:43.589393, with loss of 4.080917\n",
      "These epochs=1078) took: 0:19:55.405907, with loss of 2.1177025\n",
      "These epochs=1079) took: 0:20:07.397091, with loss of 2.1640413\n",
      "These epochs=1080) took: 0:20:19.403935, with loss of 5.1449995\n",
      "These epochs=1081) took: 0:20:31.420444, with loss of 2.08254\n",
      "These epochs=1082) took: 0:20:43.421315, with loss of 4.1458144\n",
      "These epochs=1083) took: 0:20:55.336805, with loss of 3.1412807\n",
      "These epochs=1084) took: 0:21:07.222840, with loss of 4.097412\n",
      "These epochs=1085) took: 0:21:19.159922, with loss of 3.1169064\n",
      "These epochs=1086) took: 0:21:31.070446, with loss of 3.2226262\n",
      "These epochs=1087) took: 0:21:43.043065, with loss of 4.158475\n",
      "These epochs=1088) took: 0:21:55.029021, with loss of 6.0856586\n",
      "These epochs=1089) took: 0:22:07.069214, with loss of 6.195791\n",
      "These epochs=1090) took: 0:22:19.027217, with loss of 4.099236\n",
      "These epochs=1091) took: 0:22:31.075855, with loss of 3.0857823\n",
      "These epochs=1092) took: 0:22:43.149167, with loss of 3.2114577\n",
      "These epochs=1093) took: 0:22:55.049344, with loss of 1.2227362\n",
      "These epochs=1094) took: 0:23:07.140472, with loss of 3.1302412\n",
      "These epochs=1095) took: 0:23:19.314551, with loss of 1.1516352\n",
      "These epochs=1096) took: 0:23:31.293155, with loss of 2.1529295\n",
      "These epochs=1097) took: 0:23:43.374602, with loss of 5.0559516\n",
      "These epochs=1098) took: 0:23:55.406815, with loss of 1.1016842\n",
      "These epochs=1099) took: 0:24:07.570652, with loss of 2.0911179\n",
      "These epochs=1100) took: 0:24:19.594782, with loss of 4.0425434\n",
      "These epochs=1101) took: 0:24:31.660425, with loss of 3.1003065\n",
      "These epochs=1102) took: 0:24:43.666433, with loss of 3.068613\n",
      "These epochs=1103) took: 0:24:55.572784, with loss of 3.0991936\n",
      "These epochs=1104) took: 0:25:07.537857, with loss of 3.0752776\n",
      "These epochs=1105) took: 0:25:19.517298, with loss of 3.223269\n",
      "These epochs=1106) took: 0:25:31.553837, with loss of 4.1464314\n",
      "These epochs=1107) took: 0:25:43.624500, with loss of 5.1762304\n",
      "These epochs=1108) took: 0:25:55.574638, with loss of 1.1185286\n",
      "These epochs=1109) took: 0:26:07.626569, with loss of 3.0815678\n",
      "These epochs=1110) took: 0:26:19.628318, with loss of 4.0985246\n",
      "These epochs=1111) took: 0:26:31.572117, with loss of 3.1218905\n",
      "These epochs=1112) took: 0:26:43.624177, with loss of 5.1415095\n",
      "These epochs=1113) took: 0:26:55.599503, with loss of 5.08099\n",
      "These epochs=1114) took: 0:27:07.517856, with loss of 3.0795944\n",
      "These epochs=1115) took: 0:27:19.717824, with loss of 2.1379402\n",
      "These epochs=1116) took: 0:27:31.628816, with loss of 3.045463\n",
      "These epochs=1117) took: 0:27:43.659867, with loss of 3.1647856\n",
      "These epochs=1118) took: 0:27:55.622776, with loss of 3.0582824\n",
      "These epochs=1119) took: 0:28:07.656594, with loss of 2.0874846\n",
      "These epochs=1120) took: 0:28:19.493675, with loss of 5.0738263\n",
      "These epochs=1121) took: 0:28:31.494964, with loss of 4.0946813\n",
      "These epochs=1122) took: 0:28:43.476473, with loss of 5.146717\n",
      "These epochs=1123) took: 0:28:55.244704, with loss of 2.2435513\n",
      "These epochs=1124) took: 0:29:07.210123, with loss of 1.0947572\n",
      "These epochs=1125) took: 0:29:18.987208, with loss of 6.0647163\n",
      "These epochs=1126) took: 0:29:30.889650, with loss of 2.0776262\n",
      "These epochs=1127) took: 0:29:42.833873, with loss of 1.1090955\n",
      "These epochs=1128) took: 0:29:54.916060, with loss of 1.0633081\n",
      "These epochs=1129) took: 0:30:06.914715, with loss of 2.0785987\n",
      "These epochs=1130) took: 0:30:19.008888, with loss of 4.126771\n",
      "These epochs=1131) took: 0:30:30.966349, with loss of 3.1172616\n",
      "These epochs=1132) took: 0:30:42.975341, with loss of 3.1508155\n",
      "These epochs=1133) took: 0:30:54.961410, with loss of 3.1046612\n",
      "These epochs=1134) took: 0:31:06.971837, with loss of 3.0947835\n",
      "These epochs=1135) took: 0:31:18.864494, with loss of 3.0540736\n",
      "These epochs=1136) took: 0:31:30.825976, with loss of 2.1548505\n",
      "These epochs=1137) took: 0:31:42.784727, with loss of 2.0667455\n",
      "These epochs=1138) took: 0:31:54.665809, with loss of 2.1210003\n",
      "These epochs=1139) took: 0:32:06.727503, with loss of 5.052699\n",
      "These epochs=1140) took: 0:32:18.639245, with loss of 5.1053715\n",
      "These epochs=1141) took: 0:32:30.776517, with loss of 2.168155\n",
      "These epochs=1142) took: 0:32:42.696710, with loss of 3.1216834\n",
      "These epochs=1143) took: 0:32:54.531665, with loss of 3.0731196\n",
      "These epochs=1144) took: 0:33:06.322708, with loss of 3.1196868\n",
      "These epochs=1145) took: 0:33:18.303529, with loss of 2.1545908\n",
      "These epochs=1146) took: 0:33:30.252586, with loss of 5.0877666\n",
      "These epochs=1147) took: 0:33:42.360811, with loss of 2.0580206\n",
      "These epochs=1148) took: 0:33:54.222161, with loss of 3.155971\n",
      "These epochs=1149) took: 0:34:06.192944, with loss of 3.073977\n",
      "These epochs=1150) took: 0:34:18.054512, with loss of 2.0699983\n",
      "These epochs=1151) took: 0:34:29.902601, with loss of 5.083238\n",
      "These epochs=1152) took: 0:34:41.771845, with loss of 2.1156735\n",
      "These epochs=1153) took: 0:34:53.647505, with loss of 4.0642743\n",
      "These epochs=1154) took: 0:35:05.520050, with loss of 3.093402\n",
      "These epochs=1155) took: 0:35:17.419852, with loss of 3.1003485\n",
      "These epochs=1156) took: 0:35:29.271775, with loss of 2.082003\n",
      "These epochs=1157) took: 0:35:41.148468, with loss of 5.042504\n",
      "These epochs=1158) took: 0:35:53.067797, with loss of 2.1327765\n",
      "These epochs=1159) took: 0:36:04.921395, with loss of 3.0700717\n",
      "These epochs=1160) took: 0:36:16.776515, with loss of 3.0504165\n",
      "These epochs=1161) took: 0:36:28.724489, with loss of 2.177359\n",
      "These epochs=1162) took: 0:36:40.605039, with loss of 5.215141\n",
      "These epochs=1163) took: 0:36:52.485587, with loss of 4.126616\n",
      "These epochs=1164) took: 0:37:04.378834, with loss of 3.08724\n",
      "These epochs=1165) took: 0:37:16.287342, with loss of 3.0531979\n",
      "These epochs=1166) took: 0:37:28.147956, with loss of 2.1734188\n",
      "These epochs=1167) took: 0:37:40.068409, with loss of 3.1688602\n",
      "These epochs=1168) took: 0:37:52.034725, with loss of 3.065329\n",
      "These epochs=1169) took: 0:38:04.049101, with loss of 2.082525\n",
      "These epochs=1170) took: 0:38:16.065699, with loss of 3.0726535\n",
      "These epochs=1171) took: 0:38:28.014328, with loss of 4.0307856\n",
      "These epochs=1172) took: 0:38:39.992726, with loss of 3.1680117\n",
      "These epochs=1173) took: 0:38:52.063605, with loss of 2.0628934\n",
      "These epochs=1174) took: 0:39:04.021355, with loss of 3.0466416\n",
      "These epochs=1175) took: 0:39:16.001332, with loss of 4.0673294\n",
      "These epochs=1176) took: 0:39:27.919396, with loss of 2.0741909\n",
      "These epochs=1177) took: 0:39:39.843554, with loss of 2.0906777\n",
      "These epochs=1178) took: 0:39:51.854134, with loss of 4.0429063\n",
      "These epochs=1179) took: 0:40:03.874569, with loss of 3.0778384\n",
      "These epochs=1180) took: 0:40:15.916187, with loss of 3.0460863\n",
      "These epochs=1181) took: 0:40:27.997616, with loss of 2.0688963\n",
      "These epochs=1182) took: 0:40:40.077705, with loss of 2.0645425\n",
      "These epochs=1183) took: 0:40:52.074231, with loss of 4.052958\n",
      "These epochs=1184) took: 0:41:04.057718, with loss of 4.193697\n",
      "These epochs=1185) took: 0:41:16.068417, with loss of 3.166611\n",
      "These epochs=1186) took: 0:41:27.976552, with loss of 5.0370393\n",
      "These epochs=1187) took: 0:41:39.938336, with loss of 2.2006238\n",
      "These epochs=1188) took: 0:41:51.898660, with loss of 4.0513706\n",
      "These epochs=1189) took: 0:42:04.028316, with loss of 3.0876732\n",
      "These epochs=1190) took: 0:42:16.110087, with loss of 2.1670866\n",
      "These epochs=1191) took: 0:42:28.209019, with loss of 5.109173\n",
      "These epochs=1192) took: 0:42:40.350023, with loss of 4.0509014\n",
      "These epochs=1193) took: 0:42:52.369802, with loss of 1.0994732\n",
      "These epochs=1194) took: 0:43:04.307417, with loss of 1.0710453\n",
      "These epochs=1195) took: 0:43:16.281441, with loss of 2.0446193\n",
      "These epochs=1196) took: 0:43:28.182817, with loss of 3.0513704\n",
      "These epochs=1197) took: 0:43:40.297165, with loss of 2.039511\n",
      "These epochs=1198) took: 0:43:52.293499, with loss of 1.1257921\n",
      "These epochs=1199) took: 0:44:04.570549, with loss of 3.042017\n",
      "These epochs=1200) took: 0:44:16.592919, with loss of 4.0359235\n",
      "These epochs=1201) took: 0:44:28.645301, with loss of 5.1968923\n",
      "These epochs=1202) took: 0:44:40.785031, with loss of 4.130784\n",
      "These epochs=1203) took: 0:44:52.756007, with loss of 3.09501\n",
      "These epochs=1204) took: 0:45:04.731380, with loss of 3.0645022\n",
      "These epochs=1205) took: 0:45:16.765561, with loss of 1.0595766\n",
      "These epochs=1206) took: 0:45:28.717427, with loss of 3.09312\n",
      "These epochs=1207) took: 0:45:40.746054, with loss of 2.06025\n",
      "These epochs=1208) took: 0:45:52.734208, with loss of 4.0877213\n",
      "These epochs=1209) took: 0:46:04.853212, with loss of 4.081676\n",
      "These epochs=1210) took: 0:46:16.910974, with loss of 3.116846\n",
      "These epochs=1211) took: 0:46:28.962366, with loss of 3.0920835\n",
      "These epochs=1212) took: 0:46:41.076376, with loss of 5.0393176\n",
      "These epochs=1213) took: 0:46:53.081960, with loss of 2.0569296\n",
      "These epochs=1214) took: 0:47:05.157375, with loss of 3.088954\n",
      "These epochs=1215) took: 0:47:17.222210, with loss of 4.0889707\n",
      "These epochs=1216) took: 0:47:29.206876, with loss of 3.0597603\n",
      "These epochs=1217) took: 0:47:41.266398, with loss of 4.050171\n",
      "These epochs=1218) took: 0:47:53.340496, with loss of 5.030933\n",
      "These epochs=1219) took: 0:48:05.395847, with loss of 3.1032195\n",
      "These epochs=1220) took: 0:48:17.588647, with loss of 3.0574148\n",
      "These epochs=1221) took: 0:48:29.735533, with loss of 2.096891\n",
      "These epochs=1222) took: 0:48:41.829967, with loss of 1.0542636\n",
      "These epochs=1223) took: 0:48:53.864921, with loss of 1.0506663\n",
      "These epochs=1224) took: 0:49:06.038968, with loss of 3.0732021\n",
      "These epochs=1225) took: 0:49:18.222534, with loss of 3.045224\n",
      "These epochs=1226) took: 0:49:30.233354, with loss of 5.0426507\n",
      "These epochs=1227) took: 0:49:42.423144, with loss of 4.0487995\n",
      "These epochs=1228) took: 0:49:54.452317, with loss of 2.0369172\n",
      "These epochs=1229) took: 0:50:06.608452, with loss of 4.057388\n",
      "These epochs=1230) took: 0:50:18.668641, with loss of 2.0697982\n",
      "These epochs=1231) took: 0:50:30.819855, with loss of 3.0447578\n",
      "These epochs=1232) took: 0:50:42.875380, with loss of 2.1019557\n",
      "These epochs=1233) took: 0:50:54.966751, with loss of 3.0576746\n",
      "These epochs=1234) took: 0:51:07.019865, with loss of 5.045361\n",
      "These epochs=1235) took: 0:51:19.078695, with loss of 4.040306\n",
      "These epochs=1236) took: 0:51:31.093767, with loss of 6.0377483\n",
      "These epochs=1237) took: 0:51:43.211797, with loss of 2.086766\n",
      "These epochs=1238) took: 0:51:55.200216, with loss of 4.104944\n",
      "These epochs=1239) took: 0:52:07.372765, with loss of 3.053736\n",
      "These epochs=1240) took: 0:52:19.434845, with loss of 5.0584784\n",
      "These epochs=1241) took: 0:52:31.583414, with loss of 3.0467012\n",
      "These epochs=1242) took: 0:52:43.689917, with loss of 2.1091425\n",
      "These epochs=1243) took: 0:52:55.644603, with loss of 2.0639224\n",
      "These epochs=1244) took: 0:53:07.595501, with loss of 4.063232\n",
      "These epochs=1245) took: 0:53:19.541346, with loss of 2.089582\n",
      "These epochs=1246) took: 0:53:31.502227, with loss of 3.0880556\n",
      "These epochs=1247) took: 0:53:43.458489, with loss of 3.078227\n",
      "These epochs=1248) took: 0:53:55.385728, with loss of 2.0669951\n",
      "These epochs=1249) took: 0:54:07.415450, with loss of 4.067357\n",
      "These epochs=1250) took: 0:54:19.559823, with loss of 3.0780807\n",
      "These epochs=1251) took: 0:54:31.553649, with loss of 3.138819\n",
      "These epochs=1252) took: 0:54:43.508867, with loss of 6.0291395\n",
      "These epochs=1253) took: 0:54:55.446596, with loss of 5.0622354\n",
      "These epochs=1254) took: 0:55:07.408672, with loss of 3.0477986\n",
      "These epochs=1255) took: 0:55:19.461630, with loss of 5.011348\n",
      "These epochs=1256) took: 0:55:31.502356, with loss of 6.0595236\n",
      "These epochs=1257) took: 0:55:43.658979, with loss of 3.0154424\n",
      "These epochs=1258) took: 0:55:55.612771, with loss of 2.0836077\n",
      "These epochs=1259) took: 0:56:07.720836, with loss of 2.0471857\n",
      "These epochs=1260) took: 0:56:19.786522, with loss of 4.091939\n",
      "These epochs=1261) took: 0:56:31.916414, with loss of 4.0146413\n",
      "These epochs=1262) took: 0:56:44.094738, with loss of 4.0329876\n",
      "These epochs=1263) took: 0:56:56.093936, with loss of 3.0756521\n",
      "These epochs=1264) took: 0:57:08.063725, with loss of 3.090794\n",
      "These epochs=1265) took: 0:57:20.049561, with loss of 7.058277\n",
      "These epochs=1266) took: 0:57:31.977152, with loss of 2.0510244\n",
      "These epochs=1267) took: 0:57:44.108333, with loss of 3.0271742\n",
      "These epochs=1268) took: 0:57:56.193046, with loss of 3.0778053\n",
      "These epochs=1269) took: 0:58:08.172812, with loss of 1.0441489\n",
      "These epochs=1270) took: 0:58:20.194308, with loss of 3.026301\n",
      "These epochs=1271) took: 0:58:32.161910, with loss of 3.0401638\n",
      "These epochs=1272) took: 0:58:44.108156, with loss of 3.0544298\n",
      "These epochs=1273) took: 0:58:55.996427, with loss of 1.2476351\n",
      "These epochs=1274) took: 0:59:07.833531, with loss of 1.0374469\n",
      "These epochs=1275) took: 0:59:19.704333, with loss of 2.044138\n",
      "These epochs=1276) took: 0:59:31.774745, with loss of 3.0385735\n",
      "These epochs=1277) took: 0:59:43.760968, with loss of 3.0308795\n",
      "These epochs=1278) took: 0:59:55.760568, with loss of 2.0498004\n",
      "These epochs=1279) took: 1:00:07.787019, with loss of 3.0752358\n",
      "These epochs=1280) took: 1:00:19.744428, with loss of 2.0426311\n",
      "These epochs=1281) took: 1:00:31.922315, with loss of 5.0247393\n",
      "These epochs=1282) took: 1:00:43.913531, with loss of 2.1331244\n",
      "These epochs=1283) took: 1:00:56.011777, with loss of 2.0567873\n",
      "These epochs=1284) took: 1:01:07.947734, with loss of 5.023362\n",
      "These epochs=1285) took: 1:01:19.916566, with loss of 2.084184\n",
      "These epochs=1286) took: 1:01:31.878256, with loss of 5.115915\n",
      "These epochs=1287) took: 1:01:43.805128, with loss of 3.0184016\n",
      "These epochs=1288) took: 1:01:55.709894, with loss of 4.1147633\n",
      "These epochs=1289) took: 1:02:07.628916, with loss of 1.060591\n",
      "These epochs=1290) took: 1:02:19.515330, with loss of 3.0108836\n",
      "These epochs=1291) took: 1:02:31.408736, with loss of 4.02779\n",
      "These epochs=1292) took: 1:02:43.371573, with loss of 6.0361185\n",
      "These epochs=1293) took: 1:02:55.299381, with loss of 2.0578482\n",
      "These epochs=1294) took: 1:03:07.167321, with loss of 4.0272026\n",
      "These epochs=1295) took: 1:03:18.908562, with loss of 3.042891\n",
      "These epochs=1296) took: 1:03:30.753040, with loss of 4.0209503\n",
      "These epochs=1297) took: 1:03:42.610123, with loss of 3.0383236\n",
      "These epochs=1298) took: 1:03:54.429538, with loss of 4.0192156\n",
      "These epochs=1299) took: 1:04:06.442493, with loss of 2.057468\n",
      "These epochs=1300) took: 1:04:18.282532, with loss of 7.0058928\n",
      "These epochs=1301) took: 1:04:30.187320, with loss of 3.0730681\n",
      "These epochs=1302) took: 1:04:42.180420, with loss of 6.066953\n",
      "These epochs=1303) took: 1:04:53.984687, with loss of 3.0147076\n",
      "These epochs=1304) took: 1:05:05.889496, with loss of 4.0260715\n",
      "These epochs=1305) took: 1:05:17.704068, with loss of 2.0690672\n",
      "These epochs=1306) took: 1:05:29.516567, with loss of 1.0326267\n",
      "These epochs=1307) took: 1:05:41.309471, with loss of 2.0444245\n",
      "These epochs=1308) took: 1:05:53.092952, with loss of 1.0417054\n",
      "These epochs=1309) took: 1:06:04.960171, with loss of 4.035588\n",
      "These epochs=1310) took: 1:06:16.854326, with loss of 3.045169\n",
      "These epochs=1311) took: 1:06:28.710417, with loss of 3.0426412\n",
      "These epochs=1312) took: 1:06:40.512212, with loss of 2.0371103\n",
      "These epochs=1313) took: 1:06:52.508138, with loss of 5.034381\n",
      "These epochs=1314) took: 1:07:04.439517, with loss of 3.013361\n",
      "These epochs=1315) took: 1:07:16.300175, with loss of 2.0155182\n",
      "These epochs=1316) took: 1:07:28.184616, with loss of 4.056389\n",
      "These epochs=1317) took: 1:07:40.188576, with loss of 4.0600657\n",
      "These epochs=1318) took: 1:07:52.152530, with loss of 4.0216784\n",
      "These epochs=1319) took: 1:08:04.178663, with loss of 2.061701\n",
      "These epochs=1320) took: 1:08:16.228805, with loss of 5.0143576\n",
      "These epochs=1321) took: 1:08:28.153231, with loss of 5.022849\n",
      "These epochs=1322) took: 1:08:40.136127, with loss of 2.028767\n",
      "These epochs=1323) took: 1:08:52.011553, with loss of 1.020971\n",
      "These epochs=1324) took: 1:09:03.808406, with loss of 3.067029\n",
      "These epochs=1325) took: 1:09:15.679573, with loss of 4.0275517\n",
      "These epochs=1326) took: 1:09:27.505101, with loss of 3.065137\n",
      "These epochs=1327) took: 1:09:39.389102, with loss of 1.0123366\n",
      "These epochs=1328) took: 1:09:51.606819, with loss of 4.0076303\n",
      "These epochs=1329) took: 1:10:03.632700, with loss of 2.9999602\n",
      "These epochs=1330) took: 1:10:15.573973, with loss of 4.0314713\n",
      "These epochs=1331) took: 1:10:27.327944, with loss of 4.9962144\n",
      "These epochs=1332) took: 1:10:39.075365, with loss of 5.0613008\n",
      "These epochs=1333) took: 1:10:50.755771, with loss of 1.0701163\n",
      "These epochs=1334) took: 1:11:02.244322, with loss of 3.0566113\n",
      "These epochs=1335) took: 1:11:14.000038, with loss of 3.1798842\n",
      "These epochs=1336) took: 1:11:25.792045, with loss of 4.003873\n",
      "These epochs=1337) took: 1:11:37.539360, with loss of 5.0049963\n",
      "These epochs=1338) took: 1:11:49.385190, with loss of 3.038733\n",
      "These epochs=1339) took: 1:12:01.189863, with loss of 4.0196514\n",
      "These epochs=1340) took: 1:12:13.199704, with loss of 1.0925429\n",
      "These epochs=1341) took: 1:12:25.184480, with loss of 2.0176\n",
      "These epochs=1342) took: 1:12:37.039917, with loss of 2.0636055\n",
      "These epochs=1343) took: 1:12:48.985584, with loss of 3.041821\n",
      "These epochs=1344) took: 1:13:00.880092, with loss of 2.0654907\n",
      "These epochs=1345) took: 1:13:12.821922, with loss of 2.0393717\n",
      "These epochs=1346) took: 1:13:24.572887, with loss of 4.0088053\n",
      "These epochs=1347) took: 1:13:36.438476, with loss of 4.279267\n",
      "These epochs=1348) took: 1:13:48.327964, with loss of 4.0218754\n",
      "These epochs=1349) took: 1:14:00.258033, with loss of 5.04107\n",
      "These epochs=1350) took: 1:14:11.961971, with loss of 1.0337976\n",
      "These epochs=1351) took: 1:14:23.771221, with loss of 3.0176702\n",
      "These epochs=1352) took: 1:14:35.624310, with loss of 3.998744\n",
      "These epochs=1353) took: 1:14:47.568679, with loss of 2.0164769\n",
      "These epochs=1354) took: 1:14:59.360563, with loss of 2.05125\n",
      "These epochs=1355) took: 1:15:11.199018, with loss of 2.0413477\n",
      "These epochs=1356) took: 1:15:22.818324, with loss of 3.0073173\n",
      "These epochs=1357) took: 1:15:34.608740, with loss of 2.0248384\n",
      "These epochs=1358) took: 1:15:46.403479, with loss of 4.045705\n",
      "These epochs=1359) took: 1:15:58.231972, with loss of 2.093146\n",
      "These epochs=1360) took: 1:16:10.152441, with loss of 4.002707\n",
      "These epochs=1361) took: 1:16:21.982666, with loss of 2.0443\n",
      "These epochs=1362) took: 1:16:33.852439, with loss of 3.0527432\n",
      "These epochs=1363) took: 1:16:45.767651, with loss of 3.0084324\n",
      "These epochs=1364) took: 1:16:57.741004, with loss of 2.0127165\n",
      "These epochs=1365) took: 1:17:09.531619, with loss of 5.009263\n",
      "These epochs=1366) took: 1:17:21.287893, with loss of 5.053359\n",
      "These epochs=1367) took: 1:17:33.106815, with loss of 3.9953551\n",
      "These epochs=1368) took: 1:17:45.008671, with loss of 4.0377026\n",
      "These epochs=1369) took: 1:17:56.895970, with loss of 1.997369\n",
      "These epochs=1370) took: 1:18:08.909034, with loss of 2.020858\n",
      "These epochs=1371) took: 1:18:20.678574, with loss of 2.0090683\n",
      "These epochs=1372) took: 1:18:32.737614, with loss of 1.024262\n",
      "These epochs=1373) took: 1:18:44.804610, with loss of 4.037333\n",
      "These epochs=1374) took: 1:18:56.776654, with loss of 4.422467\n",
      "These epochs=1375) took: 1:19:08.862348, with loss of 3.0225556\n",
      "These epochs=1376) took: 1:19:20.946201, with loss of 1.0250849\n",
      "These epochs=1377) took: 1:19:32.906939, with loss of 2.0231328\n",
      "These epochs=1378) took: 1:19:44.748542, with loss of 3.2834241\n",
      "These epochs=1379) took: 1:19:56.686213, with loss of 3.0476782\n",
      "These epochs=1380) took: 1:20:08.687762, with loss of 3.0564964\n",
      "These epochs=1381) took: 1:20:21.012825, with loss of 4.008299\n",
      "These epochs=1382) took: 1:20:32.797644, with loss of 4.0475936\n",
      "These epochs=1383) took: 1:20:44.867683, with loss of 3.0521047\n",
      "These epochs=1384) took: 1:20:56.704887, with loss of 3.0376465\n",
      "These epochs=1385) took: 1:21:08.490842, with loss of 3.007582\n",
      "These epochs=1386) took: 1:21:20.384246, with loss of 4.9981747\n",
      "These epochs=1387) took: 1:21:32.246696, with loss of 3.0137904\n",
      "These epochs=1388) took: 1:21:44.250313, with loss of 4.0712743\n",
      "These epochs=1389) took: 1:21:56.025631, with loss of 1.0326477\n",
      "These epochs=1390) took: 1:22:07.874257, with loss of 4.0013266\n",
      "These epochs=1391) took: 1:22:19.843804, with loss of 3.0751038\n",
      "These epochs=1392) took: 1:22:31.775911, with loss of 2.039301\n",
      "These epochs=1393) took: 1:22:43.675965, with loss of 1.0019472\n",
      "These epochs=1394) took: 1:22:55.554103, with loss of 4.011136\n",
      "These epochs=1395) took: 1:23:07.380082, with loss of 2.992548\n",
      "These epochs=1396) took: 1:23:19.448297, with loss of 4.0098104\n",
      "These epochs=1397) took: 1:23:31.355552, with loss of 4.032636\n",
      "These epochs=1398) took: 1:23:43.365911, with loss of 2.0594544\n",
      "These epochs=1399) took: 1:23:55.302378, with loss of 0.9909011\n",
      "These epochs=1400) took: 1:24:07.262189, with loss of 3.0614548\n",
      "These epochs=1401) took: 1:24:19.206436, with loss of 4.0015135\n",
      "These epochs=1402) took: 1:24:31.075789, with loss of 4.9939632\n",
      "These epochs=1403) took: 1:24:43.267117, with loss of 2.0221555\n",
      "These epochs=1404) took: 1:24:55.088302, with loss of 4.27263\n",
      "These epochs=1405) took: 1:25:07.035932, with loss of 2.0185614\n",
      "These epochs=1406) took: 1:25:19.052481, with loss of 3.03051\n",
      "These epochs=1407) took: 1:25:31.034011, with loss of 2.0162618\n",
      "These epochs=1408) took: 1:25:42.997087, with loss of 4.05709\n",
      "These epochs=1409) took: 1:25:54.969137, with loss of 3.1169693\n",
      "These epochs=1410) took: 1:26:06.946764, with loss of 1.9980462\n",
      "These epochs=1411) took: 1:26:18.854090, with loss of 4.0076256\n",
      "These epochs=1412) took: 1:26:30.742139, with loss of 4.032398\n",
      "These epochs=1413) took: 1:26:42.765477, with loss of 3.0081327\n",
      "These epochs=1414) took: 1:26:54.574936, with loss of 2.016118\n",
      "These epochs=1415) took: 1:27:06.458897, with loss of 5.991703\n",
      "These epochs=1416) took: 1:27:18.396738, with loss of 2.995967\n",
      "These epochs=1417) took: 1:27:30.323909, with loss of 4.027815\n",
      "These epochs=1418) took: 1:27:42.297682, with loss of 3.063239\n",
      "These epochs=1419) took: 1:27:54.244570, with loss of 2.9991264\n",
      "These epochs=1420) took: 1:28:06.223228, with loss of 2.987832\n",
      "These epochs=1421) took: 1:28:18.275843, with loss of 5.039189\n",
      "These epochs=1422) took: 1:28:30.355150, with loss of 2.0212584\n",
      "These epochs=1423) took: 1:28:42.301306, with loss of 2.0074856\n",
      "These epochs=1424) took: 1:28:54.218614, with loss of 2.0496168\n",
      "These epochs=1425) took: 1:29:06.031158, with loss of 1.9969372\n",
      "These epochs=1426) took: 1:29:18.002888, with loss of 3.994153\n",
      "These epochs=1427) took: 1:29:29.792773, with loss of 5.0171404\n",
      "These epochs=1428) took: 1:29:41.692249, with loss of 2.9730077\n",
      "These epochs=1429) took: 1:29:53.660229, with loss of 1.0005131\n",
      "These epochs=1430) took: 1:30:05.583438, with loss of 3.012834\n",
      "These epochs=1431) took: 1:30:17.502518, with loss of 4.032869\n",
      "These epochs=1432) took: 1:30:29.484630, with loss of 2.9762607\n",
      "These epochs=1433) took: 1:30:41.446097, with loss of 3.008514\n",
      "These epochs=1434) took: 1:30:53.618694, with loss of 3.9914374\n",
      "These epochs=1435) took: 1:31:05.516854, with loss of 3.0220895\n",
      "These epochs=1436) took: 1:31:17.377884, with loss of 1.0211577\n",
      "These epochs=1437) took: 1:31:29.238363, with loss of 4.028297\n",
      "These epochs=1438) took: 1:31:41.140439, with loss of 4.1083374\n",
      "These epochs=1439) took: 1:31:52.960833, with loss of 3.0070853\n",
      "These epochs=1440) took: 1:32:04.954290, with loss of 0.9999011\n",
      "These epochs=1441) took: 1:32:16.901915, with loss of 3.9956942\n",
      "These epochs=1442) took: 1:32:28.848811, with loss of 2.9939528\n",
      "These epochs=1443) took: 1:32:40.838440, with loss of 4.0335484\n",
      "These epochs=1444) took: 1:32:52.801036, with loss of 3.0120814\n",
      "These epochs=1445) took: 1:33:04.626445, with loss of 4.0275655\n",
      "These epochs=1446) took: 1:33:16.529346, with loss of 2.0159168\n",
      "These epochs=1447) took: 1:33:28.572965, with loss of 2.9918165\n",
      "These epochs=1448) took: 1:33:40.490935, with loss of 3.0123894\n",
      "These epochs=1449) took: 1:33:52.357814, with loss of 1.9813714\n",
      "These epochs=1450) took: 1:34:04.228201, with loss of 2.015496\n",
      "These epochs=1451) took: 1:34:16.264969, with loss of 2.983222\n",
      "These epochs=1452) took: 1:34:28.149913, with loss of 2.9905272\n",
      "These epochs=1453) took: 1:34:40.174210, with loss of 3.1158037\n",
      "These epochs=1454) took: 1:34:52.078155, with loss of 1.078034\n",
      "These epochs=1455) took: 1:35:04.030662, with loss of 4.0013247\n",
      "These epochs=1456) took: 1:35:15.934576, with loss of 2.01028\n",
      "These epochs=1457) took: 1:35:27.928677, with loss of 4.99366\n",
      "These epochs=1458) took: 1:35:39.989125, with loss of 2.0167298\n",
      "These epochs=1459) took: 1:35:51.980167, with loss of 3.997568\n",
      "These epochs=1460) took: 1:36:03.900192, with loss of 2.0520809\n",
      "These epochs=1461) took: 1:36:15.900218, with loss of 5.0119543\n",
      "These epochs=1462) took: 1:36:27.831367, with loss of 2.008522\n",
      "These epochs=1463) took: 1:36:39.768713, with loss of 4.0113773\n",
      "These epochs=1464) took: 1:36:51.678189, with loss of 2.9833317\n",
      "These epochs=1465) took: 1:37:03.626436, with loss of 2.0030527\n",
      "These epochs=1466) took: 1:37:15.546553, with loss of 3.0414343\n",
      "These epochs=1467) took: 1:37:27.477932, with loss of 2.9784904\n",
      "These epochs=1468) took: 1:37:39.381650, with loss of 0.99502176\n",
      "These epochs=1469) took: 1:37:51.314242, with loss of 3.003337\n",
      "These epochs=1470) took: 1:38:03.277952, with loss of 2.996864\n",
      "These epochs=1471) took: 1:38:15.054527, with loss of 0.97891146\n",
      "These epochs=1472) took: 1:38:26.984026, with loss of 0.9934553\n",
      "These epochs=1473) took: 1:38:38.859086, with loss of 3.9842858\n",
      "These epochs=1474) took: 1:38:50.732247, with loss of 1.983707\n",
      "These epochs=1475) took: 1:39:02.620146, with loss of 2.976023\n",
      "These epochs=1476) took: 1:39:14.532607, with loss of 2.1665845\n",
      "These epochs=1477) took: 1:39:26.406593, with loss of 2.0518236\n",
      "These epochs=1478) took: 1:39:38.332497, with loss of 7.045194\n",
      "These epochs=1479) took: 1:39:50.341024, with loss of 2.9760861\n",
      "These epochs=1480) took: 1:40:02.327177, with loss of 2.9809527\n",
      "These epochs=1481) took: 1:40:14.479877, with loss of 2.0138035\n",
      "These epochs=1482) took: 1:40:26.466895, with loss of 3.0117166\n",
      "These epochs=1483) took: 1:40:38.306570, with loss of 3.0121946\n",
      "These epochs=1484) took: 1:40:50.257906, with loss of 2.0014377\n",
      "These epochs=1485) took: 1:41:02.229687, with loss of 2.9853988\n",
      "These epochs=1486) took: 1:41:14.073555, with loss of 4.00269\n",
      "These epochs=1487) took: 1:41:26.137176, with loss of 3.052422\n",
      "These epochs=1488) took: 1:41:38.139197, with loss of 2.9862452\n",
      "These epochs=1489) took: 1:41:50.097779, with loss of 3.9872484\n",
      "These epochs=1490) took: 1:42:02.141760, with loss of 2.9815407\n",
      "These epochs=1491) took: 1:42:14.082237, with loss of 5.993725\n",
      "These epochs=1492) took: 1:42:26.049404, with loss of 2.986858\n",
      "These epochs=1493) took: 1:42:38.106328, with loss of 2.9812665\n",
      "These epochs=1494) took: 1:42:50.169372, with loss of 2.994829\n",
      "These epochs=1495) took: 1:43:02.184681, with loss of 3.0013344\n",
      "These epochs=1496) took: 1:43:14.295855, with loss of 3.9877505\n",
      "These epochs=1497) took: 1:43:26.279598, with loss of 2.9827724\n",
      "These epochs=1498) took: 1:43:38.413929, with loss of 2.0468621\n",
      "These epochs=1499) took: 1:43:50.527673, with loss of 0.9993321\n",
      "These epochs=1500) took: 1:44:02.685631, with loss of 2.968575\n",
      "These epochs=1501) took: 1:44:14.799671, with loss of 0.9899163\n",
      "These epochs=1502) took: 1:44:26.925400, with loss of 2.9756677\n",
      "These epochs=1503) took: 1:44:39.087042, with loss of 3.0534902\n",
      "These epochs=1504) took: 1:44:51.147420, with loss of 2.041089\n",
      "These epochs=1505) took: 1:45:03.333386, with loss of 3.9798517\n",
      "These epochs=1506) took: 1:45:15.481832, with loss of 2.9812636\n",
      "These epochs=1507) took: 1:45:27.537548, with loss of 3.9955115\n",
      "These epochs=1508) took: 1:45:39.683536, with loss of 2.9943602\n",
      "These epochs=1509) took: 1:45:51.802657, with loss of 3.0555248\n",
      "These epochs=1510) took: 1:46:04.203006, with loss of 3.9978595\n",
      "These epochs=1511) took: 1:46:16.439765, with loss of 2.961429\n",
      "These epochs=1512) took: 1:46:28.695592, with loss of 2.0177996\n",
      "These epochs=1513) took: 1:46:40.900230, with loss of 1.9897754\n",
      "These epochs=1514) took: 1:46:53.043431, with loss of 4.9987683\n",
      "These epochs=1515) took: 1:47:05.163021, with loss of 4.0631757\n",
      "These epochs=1516) took: 1:47:17.313461, with loss of 1.9901822\n",
      "These epochs=1517) took: 1:47:29.446911, with loss of 1.9910066\n",
      "These epochs=1518) took: 1:47:41.628600, with loss of 2.005958\n",
      "These epochs=1519) took: 1:47:53.790545, with loss of 2.9760365\n",
      "These epochs=1520) took: 1:48:06.072428, with loss of 1.9870222\n",
      "These epochs=1521) took: 1:48:18.393305, with loss of 4.9941025\n",
      "These epochs=1522) took: 1:48:30.569579, with loss of 4.001327\n",
      "These epochs=1523) took: 1:48:42.805923, with loss of 3.9803667\n",
      "These epochs=1524) took: 1:48:54.832992, with loss of 2.971807\n",
      "These epochs=1525) took: 1:49:06.949186, with loss of 4.0058928\n",
      "These epochs=1526) took: 1:49:19.164253, with loss of 1.9777402\n",
      "These epochs=1527) took: 1:49:31.207676, with loss of 2.9861603\n",
      "These epochs=1528) took: 1:49:43.334139, with loss of 3.9682171\n",
      "These epochs=1529) took: 1:49:55.396758, with loss of 3.990175\n",
      "These epochs=1530) took: 1:50:07.485894, with loss of 1.9986608\n",
      "These epochs=1531) took: 1:50:19.568835, with loss of 5.016347\n",
      "These epochs=1532) took: 1:50:31.683239, with loss of 4.992179\n",
      "These epochs=1533) took: 1:50:43.846004, with loss of 4.98665\n",
      "These epochs=1534) took: 1:50:55.866410, with loss of 2.0131721\n",
      "These epochs=1535) took: 1:51:07.910906, with loss of 4.9814596\n",
      "These epochs=1536) took: 1:51:20.255289, with loss of 3.0185604\n",
      "These epochs=1537) took: 1:51:32.249296, with loss of 3.0861032\n",
      "These epochs=1538) took: 1:51:44.436007, with loss of 0.9831135\n",
      "These epochs=1539) took: 1:51:56.540358, with loss of 2.9825287\n",
      "These epochs=1540) took: 1:52:08.741769, with loss of 6.972111\n",
      "These epochs=1541) took: 1:52:20.973170, with loss of 3.03256\n",
      "These epochs=1542) took: 1:52:33.011365, with loss of 3.035397\n",
      "These epochs=1543) took: 1:52:44.973954, with loss of 5.011855\n",
      "These epochs=1544) took: 1:52:56.963448, with loss of 3.9785671\n",
      "These epochs=1545) took: 1:53:09.018779, with loss of 2.9715552\n",
      "These epochs=1546) took: 1:53:21.083367, with loss of 2.9735584\n",
      "These epochs=1547) took: 1:53:33.190546, with loss of 2.9880464\n",
      "These epochs=1548) took: 1:53:45.427215, with loss of 3.0030575\n",
      "These epochs=1549) took: 1:53:57.624617, with loss of 2.013327\n",
      "These epochs=1550) took: 1:54:09.747345, with loss of 3.9850216\n",
      "These epochs=1551) took: 1:54:21.850812, with loss of 2.975054\n",
      "These epochs=1552) took: 1:54:33.908548, with loss of 2.0470552\n",
      "These epochs=1553) took: 1:54:46.093583, with loss of 2.009036\n",
      "These epochs=1554) took: 1:54:58.178825, with loss of 1.9876215\n",
      "These epochs=1555) took: 1:55:10.327261, with loss of 5.0742397\n",
      "These epochs=1556) took: 1:55:22.444396, with loss of 2.965965\n",
      "These epochs=1557) took: 1:55:34.586380, with loss of 3.1601887\n",
      "These epochs=1558) took: 1:55:46.821446, with loss of 3.939835\n",
      "These epochs=1559) took: 1:55:58.909676, with loss of 2.9887643\n",
      "These epochs=1560) took: 1:56:11.090310, with loss of 4.0231824\n",
      "These epochs=1561) took: 1:56:23.367613, with loss of 5.0901403\n",
      "These epochs=1562) took: 1:56:35.541735, with loss of 2.961214\n",
      "These epochs=1563) took: 1:56:47.775457, with loss of 3.9989357\n",
      "These epochs=1564) took: 1:56:59.859687, with loss of 2.9687223\n",
      "These epochs=1565) took: 1:57:11.894394, with loss of 2.9875996\n",
      "These epochs=1566) took: 1:57:23.958318, with loss of 3.999653\n",
      "These epochs=1567) took: 1:57:36.071018, with loss of 0.9766487\n",
      "These epochs=1568) took: 1:57:48.335130, with loss of 4.9794087\n",
      "These epochs=1569) took: 1:58:00.593142, with loss of 3.9826808\n",
      "These epochs=1570) took: 1:58:12.670992, with loss of 1.9714094\n",
      "These epochs=1571) took: 1:58:24.745116, with loss of 1.9674993\n",
      "These epochs=1572) took: 1:58:36.831986, with loss of 3.9777594\n",
      "These epochs=1573) took: 1:58:49.125537, with loss of 1.9603221\n",
      "These epochs=1574) took: 1:59:01.316756, with loss of 1.9885468\n",
      "These epochs=1575) took: 1:59:13.499386, with loss of 1.986557\n",
      "These epochs=1576) took: 1:59:25.558842, with loss of 1.9930991\n",
      "These epochs=1577) took: 1:59:37.772706, with loss of 5.014451\n",
      "These epochs=1578) took: 1:59:49.795354, with loss of 4.957503\n",
      "These epochs=1579) took: 2:00:01.951145, with loss of 3.9808793\n",
      "These epochs=1580) took: 2:00:14.162738, with loss of 1.970277\n",
      "These epochs=1581) took: 2:00:26.298548, with loss of 4.96318\n",
      "These epochs=1582) took: 2:00:38.482245, with loss of 1.9663618\n",
      "These epochs=1583) took: 2:00:50.562841, with loss of 2.963132\n",
      "These epochs=1584) took: 2:01:02.601658, with loss of 3.986422\n",
      "These epochs=1585) took: 2:01:14.671910, with loss of 1.9830778\n",
      "These epochs=1586) took: 2:01:26.606742, with loss of 2.0062795\n",
      "These epochs=1587) took: 2:01:38.855849, with loss of 1.9894242\n",
      "These epochs=1588) took: 2:01:50.905486, with loss of 3.9768555\n",
      "These epochs=1589) took: 2:02:03.022098, with loss of 3.0654857\n",
      "These epochs=1590) took: 2:02:15.058325, with loss of 1.9497337\n",
      "These epochs=1591) took: 2:02:27.283461, with loss of 2.967583\n",
      "These epochs=1592) took: 2:02:39.343415, with loss of 3.0356112\n",
      "These epochs=1593) took: 2:02:51.633586, with loss of 3.953423\n",
      "These epochs=1594) took: 2:03:03.787742, with loss of 3.0613172\n",
      "These epochs=1595) took: 2:03:15.961524, with loss of 0.9593756\n",
      "These epochs=1596) took: 2:03:28.058710, with loss of 1.9750726\n",
      "These epochs=1597) took: 2:03:40.325878, with loss of 3.989522\n",
      "These epochs=1598) took: 2:03:52.406737, with loss of 2.9190135\n",
      "These epochs=1599) took: 2:04:04.725251, with loss of 1.9930587\n",
      "These epochs=1600) took: 2:04:17.121403, with loss of 4.9647503\n",
      "These epochs=1601) took: 2:04:29.444480, with loss of 4.9660687\n",
      "These epochs=1602) took: 2:04:41.746189, with loss of 2.9659803\n",
      "These epochs=1603) took: 2:04:53.922334, with loss of 2.9596193\n",
      "These epochs=1604) took: 2:05:05.996766, with loss of 4.005773\n",
      "These epochs=1605) took: 2:05:18.266064, with loss of 1.9786512\n",
      "These epochs=1606) took: 2:05:30.354826, with loss of 0.9675888\n",
      "These epochs=1607) took: 2:05:42.572847, with loss of 3.9905348\n",
      "These epochs=1608) took: 2:05:54.694580, with loss of 1.0075217\n",
      "These epochs=1609) took: 2:06:06.933965, with loss of 5.00735\n",
      "These epochs=1610) took: 2:06:19.104040, with loss of 3.0039973\n",
      "These epochs=1611) took: 2:06:31.291239, with loss of 3.9770386\n",
      "These epochs=1612) took: 2:06:43.453434, with loss of 2.961882\n",
      "These epochs=1613) took: 2:06:55.525024, with loss of 4.958818\n",
      "These epochs=1614) took: 2:07:07.764789, with loss of 2.9644375\n",
      "These epochs=1615) took: 2:07:19.794790, with loss of 0.9817128\n",
      "These epochs=1616) took: 2:07:31.825902, with loss of 4.009384\n",
      "These epochs=1617) took: 2:07:44.021837, with loss of 2.966014\n",
      "These epochs=1618) took: 2:07:56.120555, with loss of 2.9475827\n",
      "These epochs=1619) took: 2:08:08.369444, with loss of 1.9596965\n",
      "These epochs=1620) took: 2:08:20.532527, with loss of 2.9634755\n",
      "These epochs=1621) took: 2:08:32.717121, with loss of 1.9726694\n",
      "These epochs=1622) took: 2:08:44.917137, with loss of 3.9973953\n",
      "These epochs=1623) took: 2:08:56.983239, with loss of 3.9388955\n",
      "These epochs=1624) took: 2:09:09.129775, with loss of 2.0022507\n",
      "These epochs=1625) took: 2:09:21.288903, with loss of 2.9647396\n",
      "These epochs=1626) took: 2:09:33.451755, with loss of 1.9747825\n",
      "These epochs=1627) took: 2:09:45.618985, with loss of 2.9574437\n",
      "These epochs=1628) took: 2:09:57.778747, with loss of 1.9447494\n",
      "These epochs=1629) took: 2:10:09.932929, with loss of 2.960446\n",
      "These epochs=1630) took: 2:10:22.151654, with loss of 3.94952\n",
      "These epochs=1631) took: 2:10:34.347653, with loss of 1.9536201\n",
      "These epochs=1632) took: 2:10:46.457229, with loss of 1.9652282\n",
      "These epochs=1633) took: 2:10:58.594595, with loss of 3.948577\n",
      "These epochs=1634) took: 2:11:10.770307, with loss of 4.9374866\n",
      "These epochs=1635) took: 2:11:22.822160, with loss of 2.9609804\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "\n",
    "abnormal_videos_matrix = pickle.load(open(\"../c3d_predictions/Anomaly/Train_features/train_features.pkl\", \"rb\")) #dim: [n, 131072]\n",
    "normal_videos_matrix = pickle.load(open(\"../c3d_predictions/Normal/Train_features/train_features.pkl\", \"rb\")) #dim: [n, 131072]\n",
    "\n",
    "start_epoch = 981\n",
    "end_epoch = 5000\n",
    "batch_size = 32 #32 for normal and 32 for abnormal total videos 64\n",
    "\n",
    "\n",
    "output_dir='trained_models/changed_op/'\n",
    "# Output_dir is the directory where you want to save trained weights\n",
    "weights_path = output_dir + 'weightsAnomalyL1L2_' + str(start_epoch-1) + '.mat'\n",
    "# weights_path = output_dir + 'weights_L1L2_pitrupat.mat'\n",
    "# weights.mat are the model weights that you will get after (or during) that training\n",
    "model_path = output_dir + 'model_' + str(start_epoch-1) + '.json'\n",
    "\n",
    "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "# tf.config.experimental_connect_to_cluster(resolver)\n",
    "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "\n",
    "# strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "# with strategy.scope():\n",
    "#     model = create_model(64*8) #batchsize of 64 across 8 workers, 32 normal and 32 abnormal\n",
    "#     if start_epoch != 0:\n",
    "#         model.load_weights(weights_path)\n",
    "#     # adagrad = AdagradOptimizer(0.001)\n",
    "#     adagrad = tf.optimizers.Adagrad(0.001)\n",
    "#     # model.compile(loss=custom_objective, optimizer=adagrad)\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "if start_epoch != 0:\n",
    "    model = load_model(model_path)\n",
    "    model = load_weights(model, weights_path)\n",
    "\n",
    "adagrad = tf.optimizers.Adagrad(0.001)\n",
    "model.compile(loss=custom_objective, optimizer=adagrad)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "       os.makedirs(output_dir)\n",
    "\n",
    "time_before = datetime.now()\n",
    "\n",
    "abnormal_batches = int(np.ceil(len(abnormal_videos_matrix)/batch_size))\n",
    "normal_batches = int(np.ceil(len(normal_videos_matrix)/batch_size))\n",
    "\n",
    "\n",
    "abnormal_videos_matrix = np.concatenate((abnormal_videos_matrix, abnormal_videos_matrix)) #wrapping around matrix if index goes out of bounds\n",
    "normal_videos_matrix = np.concatenate((normal_videos_matrix, normal_videos_matrix))#wrapping around if index goes out of bounds\n",
    "\n",
    "steps = max(abnormal_batches, normal_batches)\n",
    "\n",
    "print(\"abnormal batches:\", abnormal_batches)\n",
    "print(\"normal batches\", normal_batches)\n",
    "\n",
    "\n",
    "for e in range(start_epoch, end_epoch):\n",
    "    abnormal_perm = generate_permutation(abnormal_batches, steps)\n",
    "    normal_perm = generate_permutation(normal_batches, steps)\n",
    "    # print(\"abnormal_perm:\", abnormal_perm)\n",
    "    # print(\"normal_perm:\", normal_perm)\n",
    "    batch_loss = None\n",
    "    for step in range(steps):\n",
    "        inputs, targets=load_dataset_Train_batch(abnormal_videos_matrix, normal_videos_matrix, abnormal_perm[step], normal_perm[step], batch_size)\n",
    "        # print(\"Inputs shape:\", inputs.shape)\n",
    "        # print(\"Targets shape\", targets.shape)\n",
    "        # model.fit(inputs,targets)\n",
    "        batch_loss = model.train_on_batch(inputs,targets)\n",
    "    \n",
    "    print (\"These epochs=\" + str(e) + \") took: \" + str(datetime.now() - time_before) + \", with loss of \" + str(batch_loss))\n",
    "    if e%20==0:\n",
    "        model_path = output_dir + 'model_' + str(e) + '.json'\n",
    "        weights_path = output_dir + 'weightsAnomalyL1L2_' + str(e) + '.mat'\n",
    "        save_model(model, model_path, weights_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pV-aX_Y0RPxY"
   },
   "outputs": [],
   "source": [
    "abnormal_videos_matrix = pickle.load(open(\"../c3d_predictions/Anomaly/Test_features/test_features.pkl\", \"rb\")) #dim: [n, 131072]\n",
    "normal_videos_matrix = pickle.load(open(\"../c3d_predictions/Normal/Test_features/test_features.pkl\", \"rb\")) #dim: [n, 131072]\n",
    "inputs, targets = load_dataset_Test_batch(abnormal_videos_matrix, normal_videos_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 992,
     "status": "ok",
     "timestamp": 1584791787962,
     "user": {
      "displayName": "VAIBHAV GOYAL",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GinDWpqhe1cS7dAHJ185tE50Yn3I6rIULUmtpCN=s64",
      "userId": "09106313311714801771"
     },
     "user_tz": -330
    },
    "id": "v5gctpLOH3UI",
    "outputId": "d6457c27-caa9-473b-df2f-82638f1afc4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num abnormal: 140\n",
      "Num normal: 150\n"
     ]
    }
   ],
   "source": [
    "print(\"Num abnormal:\", len(abnormal_videos_matrix))\n",
    "print(\"Num normal:\", len(normal_videos_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9UOQmCOr-K-X"
   },
   "outputs": [],
   "source": [
    "num_iters = 20\n",
    "output_dir='trained_models/changed_op/'\n",
    "# Output_dir is the directory where you want to save trained weights\n",
    "weights_path = output_dir + 'weightsAnomalyL1L2_' + str(num_iters) + '.mat'\n",
    "# weights_path = output_dir + 'weights_L1L2_pitrupat.mat'\n",
    "# weights.mat are the model weights that you will get after (or during) that training\n",
    "model_path = output_dir + 'model_' + str(num_iters) + '.json'\n",
    "\n",
    "\n",
    "model = load_model(model_path)\n",
    "model = load_weights(model, weights_path)\n",
    "\n",
    "predictions = model.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1041,
     "status": "ok",
     "timestamp": 1584791791898,
     "user": {
      "displayName": "VAIBHAV GOYAL",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GinDWpqhe1cS7dAHJ185tE50Yn3I6rIULUmtpCN=s64",
      "userId": "09106313311714801771"
     },
     "user_tz": -330
    },
    "id": "cIUFynmiJOW6",
    "outputId": "be5df2d4-c4d4-43cf-a1b9-9a12515db20d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9280, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XnH1k1nXK2-P"
   },
   "outputs": [],
   "source": [
    "ans=[]\n",
    "k=int(9280/32)\n",
    "for i in range(k):\n",
    "    #predictions=(predictions>0.0000001)*1\n",
    "    val=sum(predictions[i*32:(i*32+32)]>0.6)\n",
    "    if(val<32):\n",
    "        ans.append(0) #abnormal\n",
    "    else:\n",
    "        ans.append(1) #normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 998,
     "status": "ok",
     "timestamp": 1584791797439,
     "user": {
      "displayName": "VAIBHAV GOYAL",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GinDWpqhe1cS7dAHJ185tE50Yn3I6rIULUmtpCN=s64",
      "userId": "09106313311714801771"
     },
     "user_tz": -330
    },
    "id": "1uCQmt8ruCTh",
    "outputId": "b5521741-c849-4df0-f8c1-c99335b5e1b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4241379310344828"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(targets==ans)/len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OUKtq46kfvlm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vWtLFkfLxHk-"
   },
   "outputs": [],
   "source": [
    "targets = np.ones((len(abnormal_videos_matrix)*32))\n",
    "targets = np.concatenate((targets,np.zeros((len(normal_videos_matrix)*32))))\n",
    "fpr, tpr, thresholds = roc_curve(targets, predictions)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 959,
     "status": "ok",
     "timestamp": 1584792163412,
     "user": {
      "displayName": "VAIBHAV GOYAL",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GinDWpqhe1cS7dAHJ185tE50Yn3I6rIULUmtpCN=s64",
      "userId": "09106313311714801771"
     },
     "user_tz": -330
    },
    "id": "EOCgQi1Ox9St",
    "outputId": "dec373ae-4987-4f2f-d967-5cdc3202ebe1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8351470191592261"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1288,
     "status": "ok",
     "timestamp": 1584792165884,
     "user": {
      "displayName": "VAIBHAV GOYAL",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GinDWpqhe1cS7dAHJ185tE50Yn3I6rIULUmtpCN=s64",
      "userId": "09106313311714801771"
     },
     "user_tz": -330
    },
    "id": "CnLyyUAVx-ie",
    "outputId": "301b0167-d94b-4789-8282-b4779743e5f4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kO-3vRnmyRUX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "training_tpu.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
